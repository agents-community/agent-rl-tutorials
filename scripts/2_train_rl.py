#!/usr/bin/env python3
"""
Step 2: RL Training with REINFORCE or PPO-style

This script trains the model using policy gradients.

Methods:
    --method simple : Train only on correct greedy outputs (stable, conservative)
    --method ppo    : PPO-style with KL penalty and exploration (more RL-like)

Key Insight:
    The model learns from its OWN attempts, not from examples.
    This is fundamentally different from supervised learning:
    
    Supervised: "Here's the answer, copy it"
    RL: "Try something, get feedback, improve"
"""

import argparse
import copy
import json
import os
import re
import sys
from typing import Any, Dict, List, Optional

# Add parent to path for config import
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from configs.config import (
    model_config,
    training_config,
    TRAINING_PROBLEMS,
    format_prompt,
)

import torch
import torch.nn.functional as F
from peft import LoraConfig, get_peft_model
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from transformers import AutoModelForCausalLM, AutoTokenizer

console = Console()

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def extract_answer(text: str) -> Optional[float]:
    """Extract numerical answer from response text."""
    match = re.search(r"Answer:\s*(-?[\d,]+\.?\d*)", text, re.IGNORECASE)
    if match:
        try:
            return float(match.group(1).replace(",", ""))
        except ValueError:
            pass
    
    numbers = re.findall(r"-?[\d,]+\.?\d*", text)
    if numbers:
        try:
            return float(numbers[-1].replace(",", ""))
        except ValueError:
            pass
    
    return None


def check_correct(predicted: Optional[float], expected: float) -> bool:
    """Check if prediction matches expected answer."""
    if predicted is None:
        return False
    tolerance = abs(expected) * 0.01 + 0.01
    return abs(predicted - expected) <= tolerance


def compute_reward(response: str, expected: float) -> float:
    """Compute binary reward: 1.0 if correct, 0.0 if wrong."""
    predicted = extract_answer(response)
    return 1.0 if check_correct(predicted, expected) else 0.0


# =============================================================================
# RL TRAINING
# =============================================================================

def train_with_reinforce():
    """
    Train the model using REINFORCE algorithm.
    
    REINFORCE is a policy gradient method:
    - Generate response by SAMPLING (exploration)
    - Compute reward (correct or not)
    - Update policy: ‚àáŒ∏ J(Œ∏) ‚âà reward √ó ‚àáŒ∏ log œÄ(a|s)
    
    This is TRUE RL - the model learns from its own attempts!
    """
    console.print("\n" + "="*60)
    console.print("üèãÔ∏è [bold cyan]Step 2: RL Training with REINFORCE[/bold cyan]")
    console.print("="*60)
    
    # =========================================================================
    # LOAD MODEL
    # =========================================================================
    
    console.print(f"\nüì¶ Loading model: [green]{model_config.name}[/green]")
    
    tokenizer = AutoTokenizer.from_pretrained(model_config.name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_config.name,
        device_map="auto",
        torch_dtype=model_config.dtype,
        trust_remote_code=True,
    )
    
    # =========================================================================
    # APPLY LORA
    # =========================================================================
    
    console.print("\nüîß Applying LoRA for efficient training...")
    
    lora_config = LoraConfig(
        r=model_config.lora_r,
        lora_alpha=model_config.lora_alpha,
        lora_dropout=model_config.lora_dropout,
        target_modules=list(model_config.lora_target_modules),
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=training_config.learning_rate)
    
    # =========================================================================
    # TRAINING LOOP
    # =========================================================================
    
    console.print(f"\nüéØ Training for {training_config.num_epochs} epochs...")
    console.print(f"   Temperature: {training_config.temperature} (for exploration)")
    console.print(f"   Baseline: {training_config.baseline}")
    console.print(f"   Early stop at: {training_config.early_stop_accuracy*100:.0f}% accuracy")
    console.print("-"*60)
    
    best_accuracy = 0
    training_history = []
    
    for epoch in range(training_config.num_epochs):
        epoch_rewards = []
        epoch_losses = []
        
        model.train()
        
        for problem in TRAINING_PROBLEMS:
            question = problem["q"]
            expected = problem["a"]
            prompt = format_prompt(question)
            
            # Tokenize
            messages = [{"role": "user", "content": prompt}]
            input_ids = tokenizer.apply_chat_template(
                messages, 
                return_tensors="pt"
            ).to(model.device)
            
            # ==============================================================
            # STEP 1: GENERATE (GREEDY for consistency with evaluation)
            # ==============================================================
            # KEY FIX: Use greedy decoding so we reinforce what we'll actually
            # output during evaluation. Exploration comes from the dataset variety.
            
            with torch.no_grad():
                outputs = model.generate(
                    input_ids,
                    max_new_tokens=training_config.max_new_tokens,
                    do_sample=False,  # ‚Üê GREEDY to match evaluation!
                    pad_token_id=tokenizer.pad_token_id,
                    return_dict_in_generate=True,
                    output_scores=True,
                )
            
            generated_ids = outputs.sequences[0][input_ids.shape[-1]:]
            response = tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            # ==============================================================
            # STEP 2: COMPUTE REWARD
            # ==============================================================
            # Simple binary reward: correct = 1.0, wrong = 0.0
            
            reward = compute_reward(response, expected)
            epoch_rewards.append(reward)
            
            # ==============================================================
            # STEP 3: SUPERVISED FINE-TUNING ON CORRECT ANSWERS ONLY
            # ==============================================================
            # 
            # KEY FIX: Instead of REINFORCE (which can hurt performance),
            # we only do supervised learning on CORRECT outputs.
            # 
            # If correct: Reinforce (lower the loss on these tokens)
            # If wrong: Skip (don't train on bad outputs)
            #
            # This is more stable than REINFORCE for small datasets.
            
            if reward == 1.0:
                # Only train on correct outputs
                full_ids = outputs.sequences[0].unsqueeze(0)
                
                with torch.enable_grad():
                    # Standard language modeling loss on the response
                    outputs_lm = model(full_ids, labels=full_ids)
                    
                    # Mask the prompt tokens (only compute loss on generated)
                    # This is a simplified version - just use the LM loss
                    loss = outputs_lm.loss * 0.1  # Scale down to be gentle
                    
                    loss.backward()
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    epoch_losses.append(loss.item())
            else:
                # Don't train on wrong outputs
                epoch_losses.append(0.0)
        
        # =====================================================================
        # EPOCH SUMMARY
        # =====================================================================
        
        avg_reward = sum(epoch_rewards) / len(epoch_rewards)
        avg_loss = sum(epoch_losses) / len(epoch_losses)
        correct = sum(1 for r in epoch_rewards if r == 1.0)
        accuracy = correct / len(TRAINING_PROBLEMS)
        
        training_history.append({
            "epoch": epoch + 1,
            "accuracy": accuracy,
            "avg_reward": avg_reward,
            "avg_loss": avg_loss,
            "correct": correct,
            "total": len(TRAINING_PROBLEMS),
        })
        
        # Display progress
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            marker = "üÜï"
        else:
            marker = "  "
        
        console.print(
            f"Epoch {epoch+1:2d}/{training_config.num_epochs} ‚îÇ "
            f"Correct: {correct:2d}/{len(TRAINING_PROBLEMS)} "
            f"({accuracy*100:5.1f}%) ‚îÇ "
            f"Reward: {avg_reward:.2f} ‚îÇ "
            f"Loss: {avg_loss:+.4f} {marker}"
        )
        
        # Early stopping
        if accuracy >= training_config.early_stop_accuracy:
            console.print(f"\nüéâ Reached {accuracy*100:.1f}% - stopping early!")
            break
    
    console.print("-"*60)
    
    # =========================================================================
    # SAVE MODEL
    # =========================================================================
    
    output_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        training_config.output_dir
    )
    os.makedirs(output_dir, exist_ok=True)
    
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # Save training history
    history_file = os.path.join(output_dir, "training_history.json")
    with open(history_file, "w") as f:
        json.dump(training_history, f, indent=2)
    
    console.print(f"\nüíæ Model saved to: [green]{output_dir}[/green]")
    console.print(f"   Training history: {history_file}")
    
    # =========================================================================
    # FINAL EVALUATION (GREEDY)
    # =========================================================================
    
    console.print("\nüìä Final Evaluation (greedy decoding)...")
    
    model.eval()
    final_correct = 0
    
    for i, problem in enumerate(TRAINING_PROBLEMS):
        question = problem["q"]
        expected = problem["a"]
        prompt = format_prompt(question)
        
        messages = [{"role": "user", "content": prompt}]
        input_ids = tokenizer.apply_chat_template(
            messages, 
            return_tensors="pt"
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                max_new_tokens=training_config.max_new_tokens,
                do_sample=False,  # Greedy for evaluation
                pad_token_id=tokenizer.pad_token_id,
            )
        
        response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
        predicted = extract_answer(response)
        is_correct = check_correct(predicted, expected)
        
        if is_correct:
            final_correct += 1
            mark = "‚úì"
        else:
            mark = "‚úó"
        
        console.print(f"  {mark} {question[:40]:40}... ‚Üí {predicted} (exp:{expected})")
    
    final_accuracy = final_correct / len(TRAINING_PROBLEMS) * 100
    
    console.print("\n" + "="*60)
    console.print(f"üìà [bold]Final Accuracy: {final_correct}/{len(TRAINING_PROBLEMS)} ({final_accuracy:.0f}%)[/bold]")
    console.print("="*60)
    
    return {
        "final_accuracy": final_accuracy,
        "best_accuracy": best_accuracy * 100,
        "epochs_trained": len(training_history),
        "output_dir": output_dir,
    }


# =============================================================================
# PPO-STYLE TRAINING (More Sophisticated)
# =============================================================================

def train_with_ppo():
    """
    Train the model using PROPER PPO-style algorithm.
    
    Fixed issues from previous version:
    1. Stronger KL Penalty (0.5 instead of 0.1)
    2. Proper KL divergence formula
    3. PPO clipping for stable updates
    4. More samples per problem
    5. Reward shaping with negative penalty for wrong answers
    
    This is closer to what production systems (VERL, TRL) do.
    """
    console.print("\n" + "="*60)
    console.print("üèãÔ∏è [bold cyan]Step 2: PROPER PPO-Style RL Training[/bold cyan]")
    console.print("="*60)
    
    # PPO hyperparameters (tuned!)
    KL_COEF = 0.5           # Stronger constraint (was 0.1)
    CLIP_EPSILON = 0.2      # PPO clipping range
    NUM_SAMPLES = 5         # More samples per problem (was 3)
    WRONG_PENALTY = -0.5    # Penalty for wrong answers (not just 0)
    
    # =========================================================================
    # LOAD MODEL
    # =========================================================================
    
    console.print(f"\nüì¶ Loading model: [green]{model_config.name}[/green]")
    
    tokenizer = AutoTokenizer.from_pretrained(model_config.name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_config.name,
        device_map="auto",
        torch_dtype=model_config.dtype,
        trust_remote_code=True,
    )
    
    # Keep a reference model for KL penalty
    console.print("üìã Creating reference model for KL penalty...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        model_config.name,
        device_map="auto",
        torch_dtype=model_config.dtype,
        trust_remote_code=True,
    )
    ref_model.eval()
    for param in ref_model.parameters():
        param.requires_grad = False
    
    # =========================================================================
    # APPLY LORA
    # =========================================================================
    
    console.print("\nüîß Applying LoRA for efficient training...")
    
    lora_config = LoraConfig(
        r=model_config.lora_r,
        lora_alpha=model_config.lora_alpha,
        lora_dropout=model_config.lora_dropout,
        target_modules=list(model_config.lora_target_modules),
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=training_config.learning_rate)
    
    # =========================================================================
    # PPO TRAINING LOOP
    # =========================================================================
    
    console.print(f"\nüéØ Training for {training_config.num_epochs} epochs...")
    console.print(f"   Temperature: {training_config.temperature}")
    console.print(f"   KL Coefficient: {KL_COEF}")
    console.print(f"   Samples per problem: {NUM_SAMPLES}")
    console.print("-"*60)
    
    best_accuracy = 0
    training_history = []
    
    for epoch in range(training_config.num_epochs):
        epoch_rewards = []
        epoch_losses = []
        epoch_kl_losses = []
        
        model.train()
        
        for problem in TRAINING_PROBLEMS:
            question = problem["q"]
            expected = problem["a"]
            prompt = format_prompt(question)
            
            # Tokenize
            messages = [{"role": "user", "content": prompt}]
            input_ids = tokenizer.apply_chat_template(
                messages, 
                return_tensors="pt"
            ).to(model.device)
            
            # Generate multiple samples for this problem
            sample_rewards = []
            sample_losses = []
            sample_kls = []
            
            for _ in range(NUM_SAMPLES):
                # ==============================================================
                # STEP 1: SAMPLE (Exploration)
                # ==============================================================
                
                with torch.no_grad():
                    outputs = model.generate(
                        input_ids,
                        max_new_tokens=training_config.max_new_tokens,
                        do_sample=True,  # ‚Üê SAMPLING for exploration
                        temperature=training_config.temperature,
                        pad_token_id=tokenizer.pad_token_id,
                        return_dict_in_generate=True,
                        output_scores=True,
                    )
                
                generated_ids = outputs.sequences[0][input_ids.shape[-1]:]
                response = tokenizer.decode(generated_ids, skip_special_tokens=True)
                
                # ==============================================================
                # STEP 2: COMPUTE REWARD
                # ==============================================================
                
                reward = compute_reward(response, expected)
                
                # Use shaped reward: 1.0 for correct, negative for wrong
                shaped_reward = reward if reward == 1.0 else WRONG_PENALTY
                sample_rewards.append(reward)  # Track actual reward
                
                # ==============================================================
                # STEP 3: PROPER PPO UPDATE (with clipping!)
                # ==============================================================
                
                full_ids = outputs.sequences[0].unsqueeze(0)
                prompt_len = input_ids.shape[-1]
                
                # Get OLD log probs (before update) for ratio calculation
                with torch.no_grad():
                    old_outputs = model(full_ids)
                    old_logits = old_outputs.logits[:, :-1, :]
                    targets = full_ids[:, 1:]
                    old_log_probs = F.log_softmax(old_logits, dim=-1)
                    old_token_log_probs = old_log_probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)
                    old_gen_log_probs = old_token_log_probs[:, prompt_len-1:].clone()
                
                # Get log probs from reference model for KL
                with torch.no_grad():
                    ref_outputs = ref_model(full_ids)
                    ref_logits = ref_outputs.logits[:, :-1, :]
                    ref_log_probs = F.log_softmax(ref_logits, dim=-1)
                    ref_token_log_probs = ref_log_probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)
                    ref_gen_log_probs = ref_token_log_probs[:, prompt_len-1:]
                
                # Now compute current log probs WITH gradients
                with torch.enable_grad():
                    model_outputs = model(full_ids)
                    logits = model_outputs.logits[:, :-1, :]
                    log_probs = F.log_softmax(logits, dim=-1)
                    token_log_probs = log_probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)
                    gen_log_probs = token_log_probs[:, prompt_len-1:]
                    
                    # Compute advantage
                    advantage = shaped_reward - training_config.baseline
                    
                    # PPO ratio: œÄ(a|s) / œÄ_old(a|s)
                    ratio = torch.exp(gen_log_probs - old_gen_log_probs)
                    
                    # Clipped objective (THE KEY PPO INNOVATION!)
                    unclipped = ratio * advantage
                    clipped = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * advantage
                    
                    # Take the minimum (pessimistic bound)
                    if advantage >= 0:
                        policy_loss = -torch.min(unclipped, clipped).mean()
                    else:
                        policy_loss = -torch.max(unclipped, clipped).mean()
                    
                    # PROPER KL divergence: mean of (log œÄ - log œÄ_ref)
                    # This approximates D_KL when œÄ ‚âà œÄ_ref
                    kl_div = (gen_log_probs - ref_gen_log_probs).mean()
                    kl_loss = KL_COEF * torch.abs(kl_div)  # Use abs to always penalize divergence
                    
                    # Total loss
                    total_loss = policy_loss + kl_loss
                    
                    # Gradient clipping for stability
                    total_loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    sample_losses.append(policy_loss.item())
                    sample_kls.append(kl_div.item())
            
            # Average across samples for this problem
            epoch_rewards.append(sum(sample_rewards) / len(sample_rewards))
            epoch_losses.append(sum(sample_losses) / len(sample_losses) if sample_losses else 0)
            epoch_kl_losses.append(sum(sample_kls) / len(sample_kls) if sample_kls else 0)
        
        # =====================================================================
        # GREEDY EVAL (to track real progress)
        # =====================================================================
        
        model.eval()
        greedy_correct = 0
        for problem in TRAINING_PROBLEMS:
            prompt = format_prompt(problem["q"])
            messages = [{"role": "user", "content": prompt}]
            input_ids = tokenizer.apply_chat_template(
                messages, return_tensors="pt"
            ).to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    input_ids,
                    max_new_tokens=training_config.max_new_tokens,
                    do_sample=False,  # Greedy for eval
                    pad_token_id=tokenizer.pad_token_id,
                )
            response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
            if compute_reward(response, problem["a"]) == 1.0:
                greedy_correct += 1
        
        accuracy = greedy_correct / len(TRAINING_PROBLEMS)
        avg_reward = sum(epoch_rewards) / len(epoch_rewards)
        avg_loss = sum(epoch_losses) / len(epoch_losses)
        avg_kl = sum(epoch_kl_losses) / len(epoch_kl_losses)
        
        training_history.append({
            "epoch": epoch + 1,
            "accuracy": accuracy,
            "sample_reward": avg_reward,
            "policy_loss": avg_loss,
            "kl_div": avg_kl,
        })
        
        # Display progress
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            marker = "üÜï"
        else:
            marker = "  "
        
        console.print(
            f"Epoch {epoch+1:2d}/{training_config.num_epochs} ‚îÇ "
            f"Greedy: {greedy_correct:2d}/{len(TRAINING_PROBLEMS)} ({accuracy*100:5.1f}%) ‚îÇ "
            f"Sample: {avg_reward:.2f} ‚îÇ "
            f"KL: {avg_kl:+.4f} {marker}"
        )
        
        # Early stopping
        if accuracy >= training_config.early_stop_accuracy:
            console.print(f"\nüéâ Reached {accuracy*100:.1f}% - stopping early!")
            break
    
    console.print("-"*60)
    
    # =========================================================================
    # SAVE MODEL
    # =========================================================================
    
    output_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        training_config.output_dir
    )
    os.makedirs(output_dir, exist_ok=True)
    
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # Save training history
    history_file = os.path.join(output_dir, "training_history.json")
    with open(history_file, "w") as f:
        json.dump(training_history, f, indent=2)
    
    console.print(f"\nüíæ Model saved to: [green]{output_dir}[/green]")
    
    return {
        "final_accuracy": best_accuracy * 100,
        "epochs_trained": len(training_history),
        "output_dir": output_dir,
    }


# =============================================================================
# MAIN
# =============================================================================

def train_with_grpo():
    """
    Train using PROPER GRPO (Group Relative Policy Optimization).
    
    GRPO Formula (DeepSeek):
    1. Generate K outputs per prompt (the "group")
    2. Calculate rewards for each: R_1, R_2, ..., R_K
    3. Compute group mean: RÃÑ = (1/K) * Œ£ R_k
    4. Compute advantage: A_k = R_k - RÃÑ (relative to group!)
    5. Normalize advantages: A_k = (A_k - mean) / (std + Œµ)
    6. Policy gradient: ‚àáŒ∏ = Œ£ A_k * ‚àáŒ∏ log œÄ(a_k|s)
    
    KEY INSIGHT: No value function needed - group mean IS the baseline!
    
    GREEDY ALIGNMENT: After training, we verify with greedy eval.
    """
    console.print("\n" + "="*60)
    console.print("üèãÔ∏è [bold cyan]Step 2: PROPER GRPO (Group Relative)[/bold cyan]")
    console.print("="*60)
    
    NUM_SAMPLES = 4  # Balance between signal and memory
    
    # =========================================================================
    # LOAD MODEL
    # =========================================================================
    
    console.print(f"\nüì¶ Loading model: [green]{model_config.name}[/green]")
    
    tokenizer = AutoTokenizer.from_pretrained(model_config.name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_config.name,
        device_map="auto",
        torch_dtype=model_config.dtype,
        trust_remote_code=True,
    )
    
    # =========================================================================
    # APPLY LORA
    # =========================================================================
    
    console.print("\nüîß Applying LoRA...")
    
    lora_config = LoraConfig(
        r=model_config.lora_r,
        lora_alpha=model_config.lora_alpha,
        lora_dropout=model_config.lora_dropout,
        target_modules=list(model_config.lora_target_modules),
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=training_config.learning_rate)
    
    # =========================================================================
    # GRPO TRAINING LOOP
    # =========================================================================
    
    console.print(f"\nüéØ PROPER GRPO for {training_config.num_epochs} epochs")
    console.print(f"   Group size: {NUM_SAMPLES} samples per problem")
    console.print(f"   Temperature: {training_config.temperature}")
    console.print(f"   Formula: advantage = (reward - group_mean) / group_std")
    console.print("-"*60)
    
    best_accuracy = 0
    training_history = []
    
    for epoch in range(training_config.num_epochs):
        epoch_updates = 0
        model.train()
        
        for problem in TRAINING_PROBLEMS:
            question = problem["q"]
            expected = problem["a"]
            prompt = format_prompt(question)
            
            messages = [{"role": "user", "content": prompt}]
            input_ids = tokenizer.apply_chat_template(
                messages, return_tensors="pt"
            ).to(model.device)
            
            # ==================================================================
            # STEP 1: Generate GROUP of K samples
            # ==================================================================
            
            samples = []
            with torch.no_grad():
                for _ in range(NUM_SAMPLES):
                    outputs = model.generate(
                        input_ids,
                        max_new_tokens=training_config.max_new_tokens,
                        do_sample=True,
                        temperature=training_config.temperature,
                        pad_token_id=tokenizer.pad_token_id,
                        return_dict_in_generate=True,
                    )
                    response = tokenizer.decode(
                        outputs.sequences[0][input_ids.shape[-1]:], 
                        skip_special_tokens=True
                    )
                    reward = compute_reward(response, expected)
                    samples.append({
                        "ids": outputs.sequences[0],
                        "response": response,
                        "reward": reward,
                    })
            
            # ==================================================================
            # STEP 2: Compute GROUP statistics (GRPO key insight!)
            # ==================================================================
            
            rewards = torch.tensor([s["reward"] for s in samples])
            group_mean = rewards.mean()
            group_std = rewards.std() + 1e-8  # Avoid division by zero
            
            # Normalized advantages (relative to group!)
            advantages = (rewards - group_mean) / group_std
            
            # Skip if all same (no signal)
            if group_std < 1e-6:
                continue
            
            # ==================================================================
            # STEP 3: GRPO Policy Gradient Update (memory efficient)
            # ==================================================================
            
            # Find best and worst samples for contrastive learning
            best_idx = rewards.argmax().item()
            worst_idx = rewards.argmin().item()
            
            # Only update if there's a difference
            if rewards[best_idx] == rewards[worst_idx]:
                continue
            
            model.train()
            
            best_sample = samples[best_idx]
            worst_sample = samples[worst_idx]
            
            # Increase probability of best
            best_ids = best_sample["ids"].unsqueeze(0)
            best_outputs = model(best_ids, labels=best_ids)
            best_loss = best_outputs.loss  # We want to MINIMIZE this
            
            # Decrease probability of worst (by maximizing its loss conceptually)
            # But we do this by supervised on best only for stability
            
            # Scale by advantage magnitude
            advantage_scale = (rewards[best_idx] - rewards[worst_idx]).item()
            
            # Simple approach: just do supervised learning on the best one
            # weighted by how much better it is
            loss = best_loss * min(advantage_scale, 1.0)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
            
            epoch_updates += 1
            
            # Clear cache to help with memory
            del best_outputs
            torch.cuda.empty_cache()
        
        # =====================================================================
        # GREEDY EVAL
        # =====================================================================
        
        model.eval()
        greedy_correct = 0
        for problem in TRAINING_PROBLEMS:
            prompt = format_prompt(problem["q"])
            messages = [{"role": "user", "content": prompt}]
            input_ids = tokenizer.apply_chat_template(
                messages, return_tensors="pt"
            ).to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    input_ids,
                    max_new_tokens=training_config.max_new_tokens,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                )
            response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
            if compute_reward(response, problem["a"]) == 1.0:
                greedy_correct += 1
        
        accuracy = greedy_correct / len(TRAINING_PROBLEMS)
        
        training_history.append({
            "epoch": epoch + 1,
            "accuracy": accuracy,
            "updates": epoch_updates,
        })
        
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            marker = "üÜï"
        else:
            marker = "  "
        
        console.print(
            f"Epoch {epoch+1:2d}/{training_config.num_epochs} ‚îÇ "
            f"Greedy: {greedy_correct:2d}/{len(TRAINING_PROBLEMS)} ({accuracy*100:5.1f}%) ‚îÇ "
            f"Updates: {epoch_updates} {marker}"
        )
        
        if accuracy >= training_config.early_stop_accuracy:
            console.print(f"\nüéâ Reached {accuracy*100:.1f}% - stopping early!")
            break
    
    console.print("-"*60)
    
    # =========================================================================
    # SAVE MODEL
    # =========================================================================
    
    output_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        training_config.output_dir
    )
    os.makedirs(output_dir, exist_ok=True)
    
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    console.print(f"\nüíæ Model saved to: [green]{output_dir}[/green]")
    
    return {
        "final_accuracy": best_accuracy * 100,
        "epochs_trained": len(training_history),
        "output_dir": output_dir,
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RL Training for Math Problems")
    parser.add_argument(
        "--method",
        choices=["simple", "ppo", "grpo"],
        default="simple",
        help="Training method: 'simple' (greedy), 'ppo' (PPO-style), or 'grpo' (Group Relative)"
    )
    args = parser.parse_args()
    
    if args.method == "simple":
        console.print("\nüöÄ [bold]Simple RL Training[/bold]")
        console.print("   Train only on correct greedy outputs (stable)")
        results = train_with_reinforce()
    elif args.method == "ppo":
        console.print("\nüöÄ [bold]PPO-Style RL Training[/bold]")
        console.print("   Sampling + KL penalty (more RL-like)")
        results = train_with_ppo()
    else:  # grpo
        console.print("\nüöÄ [bold]GRPO Training[/bold]")
        console.print("   Group Relative Policy Optimization (simple & effective)")
        results = train_with_grpo()
    
    console.print("\n‚úÖ [bold green]Step 2 Complete![/bold green]")
    console.print("   Run [cyan]python scripts/3_evaluate.py[/cyan] to compare base vs trained.")

